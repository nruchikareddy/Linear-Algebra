# -*- coding: utf-8 -*-
"""Basics of Linear Algebra.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CXXpqGacXSdMIx6ze5H8cfrig7NOE-fN

## Basics of Linear Algebra

Import the NumPy library for Linear Algebra functions and Matplotlib for some plotting functions.
"""

import numpy as np
import matplotlib.pyplot as plt

"""### Transpose of Matrix

Matrix transpose is performed with the transpose method on a nested list or a Python array, or a higher-dimensional Numpy array.
"""

# Transpose of a Matrix (as nested list)
 a = [[1,2,3,4],[2,3,4,5]]
 b = np.transpose(a)
 print('a\n',a)
 print('b\n',b)

"""If the matrix is a NumPy array, it can be treated as an object and method T can be applied over it as follows."""

# Transpose of a Matrix (as NumPy array)
print ("Matrix and its Transpose")
a = np.array([[1,2,3,4],[2,3,4,5]])
b = a.T
print('a\n',a)
print('b\n',b)

"""The dot method of NumPy performs dot-matrix product (scalar product) for 1D or higher dimensional arrays. If the inputs are scalars (numbers), it performs multiplication."""

# scalars
 a = 5
 b = 3
 z = np.dot(a,b)
 print(z)

z = a * b
 print(z)

"""In the case of one- or higher-dimensional arrays, the inputs can be either NumPy arrays, Python arrays, Python lists or Python’s nested lists."""

# 1D arrays or vectors
 a = np.array([1,2,3])  # or a = [1,2,3]
 b = np.array([2,3,4])  # or b = [2,3,4]
 z = np.dot(a,b)
 print(z)

# 2D arrays or matrices
 a = [[1,2,3],[2,0,3],[7,-5,1]]
 b = [[3,-1,5],[-2,-6,4], [0,4,4]]
 z = np.dot(a,b)
 print(z)

"""We can obtained the same result using `np.matmul()`."""

z = np.matmul(a,b)
print(z)

"""### Numpy Arrays
A NumPy array is a Numpy object upon which the dot method can be performed as below. However, this method accepts only NumPy arrays to operate on.

"""

# convert lists into NumPy arrays
 newa = np.array(a)
 newb = np.array(b)
 z = newa.dot(newb)
 print(z)

"""### The multi_dot method

It performs dot (scalar) product with 2 or more input matrices. First and last arrays can be either 1D or 2D arrays. However, the dimensions of the matrices must suit subsequent scalar matrix multiplication.
"""

# matrices with random integers: entries ranging from -4 to 4
 a = np.random.randint(-4,4,(500,5))
 b = np.random.randint(-4,4,(5,1000))
 c = np.random.randint(-4,4,(1000,10))
 d = np.random.randint(-4,4,(10,2000))
 e = np.random.randint(-4,4,(2000,200))
 # Perform multiple matrix multiplication
 z = np.linalg.multi_dot([a,b,c,d,e])

"""The result of this method can be obtained with successive dot products of matrices but multi_dot functions in an optimized manner. It decides the order of dot multiplication to complete the entire process efficiently."""

# Commented out IPython magic to ensure Python compatibility.
#  %%time
 z = np.linalg.multi_dot([a,b,c,d,e])
 print(z, '\n')

# Commented out IPython magic to ensure Python compatibility.
#  %%time
 z = a.dot(b).dot(c).dot(d).dot(e)
 print(z, '\n')

"""#### Observe the CPU time that multi_dot consumes as against CPU time that successive dot methods consume to arrive at the same solution. Which one is more efficient, why ?

### Inner Product

The inner product is the scalar multiplication of one vector (or matrix) and the transpose of another vector (or matrix). If both arrays are 1D arrays, their dimensions should be identical. If either or both arrays are higher-dimensional, then the last dimensions of both arrays should be identical.
"""

a = np.array([[1,2,3], [4,-1,0]])
 b = np.array([6,3,2])
 z = np.inner(a,b)
 print(z)

# The same results can be obtained using the dot method as follows.

a.dot(b.T)

"""### Outer Product

Outer product is the dot product of a column vector of size Mx1 and a row vector of size 1xN. The resulting array is a matrix of size MxN.

"""

a = np.array([1,2,3,4,5])
 b = np.array([6,3,2])
 z = np.outer(a,b)
 print(z)

"""The last dimension of the second array and the second-to-last dimension of the first array should be identical to perform matrix multiplication. Further, the symbol @ is also used to perform matrix multiplication."""

# Here, 'a' matrix is 3D, which means there are 3 matrices each of 2x5 size
 # Similarly, for 'b' matrix
 # So we perform 3 matrix multiplication operations each with 2x5 and 5x3 matrices from a and b
 a = np.random.random([3,2,5])
 b = np.random.random([3,5,3])
 z = a @ b
 print(z.shape)

"""### Matrix Determinant

Matrix determinant can be calculated using the method det available in the linalg module.
"""

# generate a random integer matrix of size 3 by 3
 a = np.random.randint(1,10,[3,3])
 det = np.linalg.det(a)
 print(int(det))

"""### Matrix Inverse

Inverse of a square matrix can be derived using the inv method of the linalg module.

"""

a = np.random.randint(1,10,[3,3])
 inv = np.linalg.inv(a)
 print(a)
 print()
 print(inv)

"""### Matrix Power

Matrix Power is a general method to obtain either positive or negative powers of a given square matrix. The first negative power of a matrix is technically termed its inverse. Thus, the matrix_power method can be used to find the inverse or any power of a matrix.
"""

a = np.random.random([4,4])
 # positive powers of matrix
 a_2 = np.linalg.matrix_power(a, 2)
 a_7 = np.linalg.matrix_power(a, 7)
 # inverse of matrix
 a_inv_1 = np.linalg.matrix_power(a, -1)
 a_inv_3 = np.linalg.matrix_power(a, -3)
 print('matrix \n', a)
 print('\n matrix to the power 2\n', a_2)
 print('\n matrix to the power 7\n', a_7)
 print('\n matrix inverse \n', a_inv_1)
 print('\n matrix cubic inverse \n', a_inv_3)

"""### Eigenvalues and Eigenvectors

Eigenvalues and Eigenvectors of a matrix can be determined as follows. If Eigen values cannot be determined, the method throws an error (Eg. Singular matrix).
"""

a = np.arange(9).reshape(3,3)
 eig_val, eig_vec = np.linalg.eig(a)
 print('Eigenvalues are: \n', eig_val)
 print('\nEigenvectors are: \n', eig_vec)

# Eigenvalues alone can be determined using the method eigvals as shown below.

a = np.arange(9).reshape(3,3)
eigenvalues = np.linalg.eigvals(a)
print(eigenvalues)

"""### Traces of a Matrix

Traces of a square matrix is the summation of its diagonal elements.
"""

a = np.eye(5)
print(a)
z = np.trace(a)
print('\nTrace of matrix is: ',z)

"""### Matrix Norm

Matrix or vector norm is calculated using the norm method of the linalg module.

"""

a = np.arange(12).reshape(4,3)
z = np.linalg.norm(a)
print(a)
print('\n Frobenius Norm of above matrix:')
print(z)

"""#### Norm of Matrix

Axis-wise norm determination is also possible by specifying the axis as an integer.

"""

# Norm along axis 0
a = np.arange(12).reshape(4,3)
z = np.linalg.norm(a, axis=0)
print(z)

"""### Solving System of Equations

When we think of Linear Algebra, the system of linear equations comes to our mind first, as it is tedious, time-consuming and error-prone. NumPy solves systems of linear equations in a fraction of seconds!
"""

# Coefficient Matrix
a = np.random.randint(1,20,[4,4])
# Dependent variable vector
b = np.array([4,9,12,7])
# solution
x = np.linalg.solve(a,b)
print('Coefficient Matrix')
print(a)
print('\nDependent Variable vector')
print(b)
print('\nSolution')
print(x)

# Check for correctness
B = a.dot(x)
print(B)

# This ‘B’ array is identical to the input ‘b’ array. Hence, our solution is correct.

"""### Singular Value Decomposition

Singular Value Decomposition (SVD) is one of the great dimension-reduction algorithms in machine learning. It identifies the principal components and arranges them by rank. The top ranked components contribute greatly to the original array. Here, we explore SVD with an image to get better understanding.
"""

from skimage import data
# download a sample image
image = data.astronaut()
print(image.dtype, image.min(), image.max(), image.shape)
plt.imshow(image)
plt.show()

# Normalize the image by dividing it by the maximum value, 255 and reorder the axes to be (3,400,600).

# normalize image
img = image/255.0
# reorder the axes to have colour channels as the first dimension
img = np.transpose(img, axes=(2,0,1))

"""Perform SVD on the image. It decomposes the original image into three components: U matrix, Sigma vector, and V matrix. The Sigma vector is the diagonal entries of the Sigma matrix. Hence, it is advisable that the Sigma vector may be reformed into a diagonal matrix. It should be noted that the first dimension 3 refers to the three colour channels."""

U,S,V = np.linalg.svd(img)
print(U.shape, S.shape, V.shape)

# S matrix should have dimensions suitable for matrix multiplication
Sigma = np.zeros((3,512,512))
for i in range(3):
  np.fill_diagonal(Sigma[i,:,:], S[i,:])
print(Sigma.shape)

# Reconstruct the original image without any dimension reduction.
reconst = U @ Sigma @ V
reconst = np.transpose(reconst, axes=(1,2,0))
plt.imshow(reconst)
plt.show()

# NumPy SVD reconstruction

# Reconstruct the data by reducing the common dimensions from 512 to 50.

k = 50
reconst = U @ Sigma[:,:,:k] @ V[:,:k,:]
reconst = np.transpose(reconst, axes=(1,2,0))
plt.imshow(reconst)
plt.show()

"""#### Dimensionality reduction

It is amazing that we reconstructed the image with most details, even at a reduction of ¼.

We can once again reconstruct the same image by reducing the data points from 512 to 20.

"""

k = 20
reconst = U @ Sigma[:,:,:k] @ V[:,:k,:]
reconst = np.transpose(reconst, axes=(1,2,0))
plt.imshow(reconst)
plt.show()

"""#### SVD reconstruction

Out of 400 data points, merely 20 data points can reconstruct the image with key feature details! This is why the old mathematical algorithm- SVD is still popular in machine learning.

### Additional References

[Numpy Matrix Multiplication Terminology](https://likegeeks.com/numpy-matrix-multiplication/#Basic_Terminologies)
"""

